# Explaining GANs to your grandmother: a guide

_Generative Adversarial Networks (GANs)_ are a type of Neural Network created by Ian Goodfellow and his colleagues back in 2014. The **'Generative'** term in its name clearly indicates that this is a type of neural network that generates something. In this case, its a Neural Network that generates images. Ever wondered how?

## GANs: What are they, what do they do and how do they do it?

As you've read above, GANs are a type of Neural Network that generates new data based on the data points it was trained on, but how are they different than vanilla neural networks? GANs are not a special type of networks with special types of layers or anything else, they are just a way of making neural networks do something crazy. Let's start by analzing the name and see what we can take from it:

**Generative**: Generates stuff
**Adversarival**: Competition? One against another?
**Networks**: Neurons, Layers, Weights, Backprop & etc ...

**Generative Adversarial Networks** are a set of _two_ neural networks competing with each eachother. The **Generator** and the **Discriminator**. The first is a neural network that generates new instances of data, while the second is a different neural network that tries to differentiate original instances from AI generated instances.

The objective of the **Generator** is to generate new samples that are indistinguishable from real ones, fooling the **Discriminator**, that tries to perfectly differentiate real samples from generated ones. That's where the **Adversarial** term comes in, they are competing with eachother.

### The Generator Network

The Generator is a network that generates new samples from the same distribution of the data that it was trained on.

Our training data is a set of $n$ instances:

$$ \text{training data = }\{x^{(i)}\}_{i=0}^n $$

We want to generate $h$ new samples:

$$ \text{new samples = }\{x^{(j)*}\}_{j=0}^h $$

These new $\{x^{(j)*}\}$ samples have to follow the same probability distribution as of the training data. That's important because we want the generated samples to be representative of the real world data the model was training on.

We do that by first sampling a latent random variable $ z^{(j)} $ from a know distribution, such as a normal or uniform distribution. Then, we input this latent variable into our network to obtain it's output $ x^{(j)*}$ (a new, generated sample):

$$ x^{(j)*} = g[z^{(j)}, \theta] $$

Here, $ \theta $ are the paremeters of our network.

**Don't forget**: We want our generated samples $\{x^{(j)*}\}$ to be indistinguishable from the real samples $\{x^{(i)}\}$.

In order to train our network to do that, we have a **Discriminator** network.

### The Discriminator Network

The discriminator is a different neural network that classifies a given input into **real** or **fake** (generated by the generator network). It can be represented as:

$$ f[\cdot , \phi] $$

Here, $\phi$ are the parameters for the discriminator network.

It receives an input $x^{(i)}$ and outputs a **probability** of $x^{(i)}$ being generated by the generator. If it's correct (outputs a high probability of a generated sample actually being fake), it gives a signal used to improve the generation process.

### Very Simple Example

Let's say our real data was obtained by a **normal** distribution $N(10, 4)$. We sample a random data point $x^{(i)} = [13.2, 8.7]$

In order to start ou process, we sample a latent random variable $z^{(j)}$ from a **normal** distribution $N(0, 1)$. We got  $z^{(j)} = [0.2, -0.7]$

Then, we input $z^{(j)}$ into our generator and get $x^{(j)*}$, where:

$$ x^{(j)*} = g[z^{(j)}, \theta] $$

Let's say our generator is a very simple network like this: $ x^{(j)*} = (x^{(j)} + \theta_1) * \theta_2 $

Now, we need to train our generator so $\theta_1$ and $\theta_2$ brings $x^{(i)}$ and $x^{(j)}$ as close as possible.

Starting from a random intialization where $\theta_1 = 3$ and $\theta_2 = 1$:

$$ x^{(j)*} = (z^{(j)} + \theta_1) * \theta_2 = (z^{(j)} + 3) * 1 = [3.2, 2.3]$$

Now that we have $ x^{(j)*} $, we input it into our discriminator network, that would have to classify it into **real** or **fake**. $[3.2, 2.3]$ doesn't look to have come out from a normal distribution such as $N(10, 4)$, so it would be a piece of cake for it to classify $ x^{(j)*}$ as **fake**.

Now a little time has passed, we trained our generator a little bit and got $\theta_1 = 10$ and $\theta_2 = 1.1$. We input the same $z^{(j)}$ in it and now we have this:

$$ x^{(j)*} = (z^{(j)} + \theta_1) * \theta_2 = (z^{(j)} + 10) * 2 = [11.22, 10.23]$$

Now, our discriminator will have a much harder time to classify $[11.22, 10.23]$ as **fake** when the real data looks like $[13.2, 8.7]$. They could have very much be sampled from the same distribution. That's the hole point of GANs!

## Training GANs
